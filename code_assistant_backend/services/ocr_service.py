import os
from dotenv import load_dotenv
import requests

load_dotenv()  # take environment variables from .env.
hugging_face_token = os.getenv('HUGGING_FACE_TOKEN')

# def extract_text_from_image(image_path):
#     API_URL = "https://api-inference.huggingface.co/models/kha-white/manga-ocr-base"
#     headers = {"Authorization": f"Bearer {hugging_face_token}"}
#     with open(image_path, "rb") as f:
#         data = f.read()
#     response = requests.post(API_URL, headers=headers, data=data)
#     return response.json()

# # import easyocr
# # 
# # load_dotenv()  # take environment variables from .env.
# # hugginig_face_token = os.getenv('HUGGING_FACE_TOKEN')
# # def extract_text_from_image(image_path):
# #     reader = easyocr.Reader(['en'])
# #     result = reader.readtext(image_path, detail=0)
# #     return " ".join(result)
# # 
# # 
# # # import requests
# # 
# # # API_URL = "https://api-inference.huggingface.co/models/microsoft/trocr-base-handwritten"
# # # headers = {"Authorization": "Bearer hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"}
# # 
# # # def query(filename):
# # #     with open(filename, "rb") as f:
# # #         data = f.read()
# # #     response = requests.post(API_URL, headers=headers, data=data)
# # #     return response.json()
# # 
# # # output = query("cats.jpg")



import os
import logging
from PIL import Image
import pytesseract
from huggingface_hub import InferenceClient

# Constants
# Constants
MODEL_NAME = "codellama"  # Use 'codellama' for the model name
OLLAMA_API_URL = "http://localhost:11434/api/v1/generate"  # Ollama's local API URL

# Setting up logging configuration
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Hugging Face token from environment variables
hugging_face_token = os.getenv('HUGGING_FACE_TOKEN')

def extract_text_from_image(image_path):
    """
    Extracts text from the given image using pytesseract OCR.
    
    :param image_path: str - Path to the image file.
    :return: str - Extracted text from the image.
    """
    # Check if the image file exists
    if not os.path.exists(image_path):
        logging.error(f"File {image_path} not found.")
        return None

    try:
        # Open the image file
        img = Image.open(image_path)
        logging.info(f"Image {image_path} loaded successfully.")
        
        # Perform OCR on the image
        text = pytesseract.image_to_string(img)
        # print(text)
        text2 = 'what is python?'
        logging.info("Text extraction completed.")
        
        return get_model_output(text)
        # return get_mocked_model_output(text2)

    except Exception as e:
        logging.error(f"An error occurred during text extraction: {str(e)}")
        return None


def get_model_output(prompt):
    """
    Queries the locally running Code Llama model with the given prompt and returns the output.
    
    :param prompt: str - The prompt to send to the model.
    :return: str - The output generated by the model.
    """
    # Check if prompt is valid
    if not prompt:
        logging.error("Prompt is empty. Unable to query the model.")
        return None

    try:
        # Construct the API request payload for Code Llama
        payload = {
            "model": MODEL_NAME,
            "prompt": prompt,
            "max_tokens": 500
        }

        # Send a POST request to Ollama's local API
        response = requests.post(OLLAMA_API_URL, json=payload)
        
        # Check if the request was successful
        if response.status_code == 200:
            result = response.json().get("text", "")
            logging.info("Model output generation completed successfully.")
            return result
        else:
            logging.error(f"Error querying the Code Llama model: {response.status_code} {response.text}")
            return None

    except Exception as e:
        logging.error(f"An error occurred while querying the model: {str(e)}")
        return None
    
    
def get_mocked_model_output(prompt):
    """
    Mocks the model output to simulate the behavior of an AI model.
    
    :param prompt: str - The prompt to simulate.
    :return: str - The mocked output for the provided prompt.
    """
    logging.info(f"Received prompt: {prompt}")

    # Mocked responses based on the prompt
    mocked_responses = {
        'what is python?': "Python is a high-level, interpreted programming language known for its ease of use and readability. It's widely used in web development, data science, automation, and more.",
        'what is machine learning?': "Machine learning is a field of artificial intelligence that focuses on building algorithms that learn patterns from data and make decisions with minimal human intervention.",
        'who is the president of the US?': "As of now, the President of the United States is Joe Biden (assuming this is up-to-date).",
        # Add more mocked responses as needed
    }

    # Return the mocked response or a default one
    response = mocked_responses.get(prompt.lower(), "I'm sorry, I don't have a mock response for that prompt.")
    logging.info(f"Returning mocked response: {response}")

    return response
    

# def get_model_output(model_name, token, question):
#     """
#     Queries the model with the given question and returns the output.
    
#     :param model_name: str - The name of the model to query.
#     :param token: str - The token for authentication.
#     :param question: str - The question to ask the model.
#     :return: str - The output from the model.
#     """
#     client = InferenceClient(
#         model_name,
#         token=token,
#     )

#     output = ""
#     for message in client.chat_completion(
#         messages=[{"role": "user", "content": question}],
#         max_tokens=500,
#         stream=True,
#     ):
#         output += message.choices[0].delta.content
#     return output