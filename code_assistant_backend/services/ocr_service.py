import os
from dotenv import load_dotenv
import requests

load_dotenv()  # take environment variables from .env.
hugging_face_token = os.getenv('HUGGING_FACE_TOKEN')

# def extract_text_from_image(image_path):
#     API_URL = "https://api-inference.huggingface.co/models/kha-white/manga-ocr-base"
#     headers = {"Authorization": f"Bearer {hugging_face_token}"}
#     with open(image_path, "rb") as f:
#         data = f.read()
#     response = requests.post(API_URL, headers=headers, data=data)
#     return response.json()

# # import easyocr
# # 
# # load_dotenv()  # take environment variables from .env.
# # hugginig_face_token = os.getenv('HUGGING_FACE_TOKEN')
# # def extract_text_from_image(image_path):
# #     reader = easyocr.Reader(['en'])
# #     result = reader.readtext(image_path, detail=0)
# #     return " ".join(result)
# # 
# # 
# # # import requests
# # 
# # # API_URL = "https://api-inference.huggingface.co/models/microsoft/trocr-base-handwritten"
# # # headers = {"Authorization": "Bearer hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"}
# # 
# # # def query(filename):
# # #     with open(filename, "rb") as f:
# # #         data = f.read()
# # #     response = requests.post(API_URL, headers=headers, data=data)
# # #     return response.json()
# # 
# # # output = query("cats.jpg")



import os
import logging
from PIL import Image
import pytesseract
from huggingface_hub import InferenceClient

# Constants
MODEL_NAME = "Qwen/Qwen2.5-Coder-7B-Instruct"

# Setting up logging configuration
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Hugging Face token from environment variables
hugging_face_token = os.getenv('HUGGING_FACE_TOKEN')

def extract_text_from_image(image_path):
    """
    Extracts text from the given image using pytesseract OCR.
    
    :param image_path: str - Path to the image file.
    :return: str - Extracted text from the image.
    """
    # Check if the image file exists
    if not os.path.exists(image_path):
        logging.error(f"File {image_path} not found.")
        return None

    try:
        # Open the image file
        img = Image.open(image_path)
        logging.info(f"Image {image_path} loaded successfully.")
        
        # Perform OCR on the image
        text = pytesseract.image_to_string(img)
        print(text)
        text2 = 'what is python?'
        logging.info("Text extraction completed.")
        
        return get_model_output(hugging_face_token, text2)

    except Exception as e:
        logging.error(f"An error occurred during text extraction: {str(e)}")
        return None


def get_model_output(token, prompt):
    """
    Queries the model with the given prompt and returns the output.
    
    :param token: str - The Hugging Face API token for authentication.
    :param prompt: str - The prompt to send to the model.
    :return: str - The output generated by the model.
    """
    # Validate token
    if not token:
        logging.error("Hugging Face token is missing.")
        return None

    try:
        # Initialize the inference client
        client = InferenceClient(token=token)
        logging.info(f"Successfully connected to Hugging Face with provided token.")

        output = ""
        # Query the model with the provided prompt
        for message in client.chat_completion(
            model=MODEL_NAME,  # Pass model here instead
            messages=[{"role": "user", "content": prompt}],
            max_tokens=500,
            stream=True,
        ):
            output += message.choices[0].delta.content

        if output:
            logging.info("Model output generation completed successfully.")
        else:
            logging.warning("No output generated by the model.")

        return output

    except Exception as e:
        logging.error(f"An error occurred while querying the model: {str(e)}")
        return None

    
    

# def get_model_output(model_name, token, question):
#     """
#     Queries the model with the given question and returns the output.
    
#     :param model_name: str - The name of the model to query.
#     :param token: str - The token for authentication.
#     :param question: str - The question to ask the model.
#     :return: str - The output from the model.
#     """
#     client = InferenceClient(
#         model_name,
#         token=token,
#     )

#     output = ""
#     for message in client.chat_completion(
#         messages=[{"role": "user", "content": question}],
#         max_tokens=500,
#         stream=True,
#     ):
#         output += message.choices[0].delta.content
#     return output