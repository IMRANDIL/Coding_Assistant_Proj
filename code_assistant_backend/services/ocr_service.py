import os
from dotenv import load_dotenv
import requests

load_dotenv()  # take environment variables from .env.
hugging_face_token = os.getenv('HUGGING_FACE_TOKEN')

# def extract_text_from_image(image_path):
#     API_URL = "https://api-inference.huggingface.co/models/kha-white/manga-ocr-base"
#     headers = {"Authorization": f"Bearer {hugging_face_token}"}
#     with open(image_path, "rb") as f:
#         data = f.read()
#     response = requests.post(API_URL, headers=headers, data=data)
#     return response.json()

# # import easyocr
# # 
# # load_dotenv()  # take environment variables from .env.
# # hugginig_face_token = os.getenv('HUGGING_FACE_TOKEN')
# # def extract_text_from_image(image_path):
# #     reader = easyocr.Reader(['en'])
# #     result = reader.readtext(image_path, detail=0)
# #     return " ".join(result)
# # 
# # 
# # # import requests
# # 
# # # API_URL = "https://api-inference.huggingface.co/models/microsoft/trocr-base-handwritten"
# # # headers = {"Authorization": "Bearer hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"}
# # 
# # # def query(filename):
# # #     with open(filename, "rb") as f:
# # #         data = f.read()
# # #     response = requests.post(API_URL, headers=headers, data=data)
# # #     return response.json()
# # 
# # # output = query("cats.jpg")


import os
import logging
import requests  # Missing import for making HTTP requests
from PIL import Image
import pytesseract

# Constants
MODEL_NAME = "codellama"  # Use 'codellama' for the model name
OLLAMA_API_URL = "http://localhost:11434/api/generate"  # Correct API URL

# Setting up logging configuration
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


def extract_text_from_image(image_path):
    """
    Extracts text from the given image using pytesseract OCR.
    
    :param image_path: str - Path to the image file.
    :return: str - Extracted text from the image.
    """
    # Check if the image file exists
    if not os.path.exists(image_path):
        logging.error(f"File {image_path} not found.")
        return None

    try:
        # Open the image file
        img = Image.open(image_path)
        logging.info(f"Image {image_path} loaded successfully.")
        
        # Perform OCR on the image
        extracted_text = pytesseract.image_to_string(img)
        logging.info("Text extraction completed.")
        
        # Add extracted text to the prompt template
        prompt = generate_prompt_template(extracted_text)
        
        return get_model_output(prompt)

    except Exception as e:
        logging.error(f"An error occurred during text extraction: {str(e)}")
        return None


def generate_prompt_template(extracted_text):
    """
    Generates a prompt template by adding the extracted text to a programming expert explanation prompt.
    
    :param extracted_text: str - The text extracted from the image.
    :return: str - A detailed prompt with explanations and examples.
    """
    prompt_template = f"""
    You are an expert programming instructor with in-depth knowledge of most programming concepts and their nuances.
    Please help the user understand the following code or text by explaining it line by line, using simple terms and clear examples.

    Here is the extracted text:

    {extracted_text}

    For each line, provide:
    1. A brief explanation of what it does.
    2. A simple example or analogy to clarify the concept.
    3. If possible, show how it relates to real-world scenarios or common programming tasks.
    """

    logging.info("Prompt template generated successfully.")
    return prompt_template


def get_model_output(prompt):
    """
    Queries the locally running Code Llama model with the given prompt and returns the output.
    
    :param prompt: str - The prompt to send to the model.
    :return: str - The output generated by the model.
    """
    # Check if prompt is valid
    if not prompt:
        logging.error("Prompt is empty. Unable to query the model.")
        return None

    try:
        # Construct the API request payload for Code Llama with stream set to false
        payload = {
            "model": MODEL_NAME,
            "prompt": prompt,
            "max_tokens": 500,
            "stream": False  # Disable streaming
        }

        # Send a POST request to the API
        response = requests.post(OLLAMA_API_URL, json=payload, headers={"Content-Type": "application/json"})
        
        # Log the response for debugging
        logging.info(f"Response status code: {response.status_code}")
        
        # Check if the request was successful
        if response.status_code == 200:
            result = response.json().get("response", "")
            logging.info("Model output generation completed successfully.")
            return result
        else:
            logging.error(f"Error querying the Code Llama model: {response.status_code} {response.text}")
            return None

    except Exception as e:
        logging.error(f"An error occurred while querying the model: {str(e)}")
        return None

# Example usage:
# extract_text_from_image("/path/to/your/image.png")

  
def get_mocked_model_output(prompt):
    """
    Mocks the model output to simulate the behavior of an AI model.
    
    :param prompt: str - The prompt to simulate.
    :return: str - The mocked output for the provided prompt.
    """
    logging.info(f"Received prompt: {prompt}")

    # Mocked responses based on the prompt
    mocked_responses = {
        'what is python?': "Python is a high-level, interpreted programming language known for its ease of use and readability. It's widely used in web development, data science, automation, and more.",
        'what is machine learning?': "Machine learning is a field of artificial intelligence that focuses on building algorithms that learn patterns from data and make decisions with minimal human intervention.",
        'who is the president of the US?': "As of now, the President of the United States is Joe Biden (assuming this is up-to-date).",
        # Add more mocked responses as needed
    }

    # Return the mocked response or a default one
    response = mocked_responses.get(prompt.lower(), "I'm sorry, I don't have a mock response for that prompt.")
    logging.info(f"Returning mocked response: {response}")

    return response
    

# def get_model_output(model_name, token, question):
#     """
#     Queries the model with the given question and returns the output.
    
#     :param model_name: str - The name of the model to query.
#     :param token: str - The token for authentication.
#     :param question: str - The question to ask the model.
#     :return: str - The output from the model.
#     """
#     client = InferenceClient(
#         model_name,
#         token=token,
#     )

#     output = ""
#     for message in client.chat_completion(
#         messages=[{"role": "user", "content": question}],
#         max_tokens=500,
#         stream=True,
#     ):
#         output += message.choices[0].delta.content
#     return output